üìò README ‚Äî Infraestrutura m√≠nima para acesso SSH em EC2 + Prepara√ß√£o do Ambiente Kubernetes (Etapa 6)
üìÖ Atualiza√ß√µes

16/11/25 ‚Äî Infraestrutura AWS para SSH funcional

17/11/25 ‚Äî Etapa 6: Reposit√≥rio Kubernetes + Containerd

1. üì° Infraestrutura m√≠nima para acessar uma EC2 via SSH

Este documento descreve todos os recursos necess√°rios para criar uma inst√¢ncia EC2 acess√≠vel por SSH usando Terraform.
Tamb√©m explica por que a primeira vers√£o n√£o funcionava e como os elementos de rede da AWS se conectam.

Para que uma EC2 seja acess√≠vel pela internet via SSH, s√£o obrigat√≥rios cinco componentes de rede:

‚úî 1. VPC

A VPC √© a rede privada onde todos os recursos s√£o criados.

‚úî 2. Subnet p√∫blica

Uma subnet s√≥ √© considerada p√∫blica quando possui rota para um Internet Gateway.
Sem isso, mesmo que a EC2 tenha IP p√∫blico, ela permanece isolada.

‚úî 3. Internet Gateway (IGW)

Respons√°vel por permitir tr√°fego de/para a internet.
Sem IGW ‚Üí nenhum pacote chega na EC2 ‚Üí SSH falha.

‚úî 4. Route Table

A subnet p√∫blica precisa de uma rota padr√£o:

0.0.0.0/0 ‚Üí igw-xxxxx


Sem essa rota, a EC2 n√£o ser√° acess√≠vel externamente.

‚úî 5. Security Group

O Security Group deve liberar a porta 22/tcp:

22/tcp ‚Üí seu_IP/32


0.0.0.0/0 funciona, mas √© inseguro.

2. ‚ùå Por que sua primeira configura√ß√£o n√£o funcionava

Sua primeira tentativa criava apenas o Security Group, mas n√£o:

VPC

Subnet p√∫blica

Internet Gateway

Route Table com rota para o IGW

Associa√ß√£o da Route Table com a Subnet

Mesmo com:

IP p√∫blico

Porta 22 liberada

A EC2 continuava inacess√≠vel porque:

A subnet n√£o tinha rota para internet

N√£o havia IGW associado

A EC2 estava isolada dentro da VPC

‚úî Ap√≥s a corre√ß√£o, estes recursos foram criados:

VPC

Subnet p√∫blica

Internet Gateway

Route Table com rota para o IGW

Associa√ß√£o entre Subnet e Route Table

Security Group com porta 22 liberada

EC2 conectada corretamente

Caminho final do tr√°fego SSH:

internet ‚Üí IGW ‚Üí Route Table ‚Üí Subnet p√∫blica ‚Üí EC2 (porta 22 liberada)

3. üîÅ Fluxo do tr√°fego SSH
(SEU PC)
   ‚Üì (22/tcp)
internet
   ‚Üì
Internet Gateway
   ‚Üì
Route Table (0.0.0.0/0 ‚Üí IGW)
   ‚Üì
Subnet p√∫blica
   ‚Üì
EC2 (SG permitindo SSH)


Se qualquer parte estiver faltando ‚Üí SSH n√£o funciona.

4. üß± Infraestrutura criada
Recursos essenciais

aws_vpc

aws_subnet

aws_internet_gateway

aws_route_table

aws_route_table_association

aws_security_group

aws_instance

Ordem l√≥gica

Criar VPC

Criar Subnet

Criar Internet Gateway

Criar Route Table

Associar Route Table √† Subnet

Criar Security Group

Criar EC2

5. ‚ö† Erros comuns que impedem SSH
Erro	Consequ√™ncia
Subnet sem rota para IGW	EC2 isolada
IGW ausente	Sem tr√°fego externo
SG sem porta 22	SSH bloqueado
Usar IP errado no SG	Conex√£o negada
EC2 em subnet privada	Sem acesso externo
6. üéØ Conclus√£o

Para conectar via SSH em uma EC2, √© fundamental configurar corretamente toda a estrutura de rede, e n√£o apenas o Security Group.

Com essa base pronta, avan√ßamos para a prepara√ß√£o da inst√¢ncia para o Kubernetes.

7. ü§ñ Adi√ß√£o do Ansible na EC2

O Ansible foi instalado automaticamente usando user-data durante a cria√ß√£o da inst√¢ncia.

8. üöÄ Etapa 6 ‚Äî Reposit√≥rio Kubernetes + Instala√ß√£o do Containerd

(17/11/25)

Esta etapa prepara a EC2 para receber os bin√°rios Kubernetes.
Configuramos o reposit√≥rio pkgs.k8s.io, instalamos o containerd e aplicamos otimiza√ß√µes recomendadas pela CNCF.

üéØ Objetivos

Registrar o reposit√≥rio oficial Kubernetes

Importar a chave GPG correta (evitando erros NO_PUBKEY)

Instalar e configurar o containerd

Ajustar par√¢metros do sistema

Habilitar o uso de SystemdCgroup

üß© Implementa√ß√µes realizadas
‚úî 1. Atualiza√ß√£o do APT

Garantimos o uso dos reposit√≥rios mais recentes.

‚úî 2. Instala√ß√£o de depend√™ncias

Incluindo ferramentas para trabalhar com GPG e reposit√≥rios HTTPS.

‚úî 3. Baixar e registrar chave GPG oficial

A chave foi armazenada em:

/etc/apt/keyrings/kubernetes-apt-keyring.gpg

‚úî 4. Criar o reposit√≥rio Kubernetes

Arquivo gerado:

/etc/apt/sources.list.d/kubernetes.list

‚úî 5. Instala√ß√£o do containerd

Container runtime recomendado para clusters Kubernetes modernos.

‚úî 6. Configura√ß√£o do containerd

Foi regenerado o arquivo:

/etc/containerd/config.toml


Com ajustes:

SystemdCgroup = true

conformidade com kubelet e CRI

‚úî 7. Rein√≠cio e habilita√ß√£o

O containerd foi reiniciado e configurado para iniciar automaticamente.


ver seu ip: https://ifconfig.me/










Projeto devops!

Parte: Infraestrutura
Motivo: Ambiente automatizado de deploy

Entregas:

-  Cria√ß√£o de rede (VPC/SUBNET/IGW/RT/SECURITY GROUP/REGRAS)
-  Cria√ß√£o de uma m√°quina via terraform com a fun√ß√£o de ser o nosso controller do Ansible (Ansible Controller)
-  Cria√ß√£o de duas m√°quinas via terraform, uma sendo um node Master do K8S e um Worker
-  Automa√ß√£o via ansible para realizar as configura√ß√µes necess√°rias nas duas maquinas do K8S


Processos:
   - Inicio:
      Rode "terraform apply" para subir os recursos

   - Ansible-controller
      Acesse a m√°quina via ssh e execute os seguintes comandos:

         mkdir -p ~/.ssh
         chmod 700 ~/.ssh
         nano ~/.ssh/id_rsa

         e ap√≥s isso cole a chave dentro do arquivo, salve e feche. Como √∫tlimo comando rode 
         chmod 600 ~/.ssh/id_rsa
      
      Isso vai permitir o acesso via ssh para os outros workers com a mesma key pair:

         - Configura√ß√£o do Ansible (Inventory)
            Execute:
            sudo nano /etc/ansible/hosts
            
            e adicione os ips privados das maquinas nesse estilo:
            [k8smaster]
            10.0.1.176 ansible_user=ubuntu

            Ap√≥s isso pode tentar realizar um pin com ansbile, tipo:
            ansible -i /etc/ansible/hosts k8smaster -m ping

            a saida de sucesso √© tipo essa:

            10.0.1.193 | SUCCESS => {
                "ansible_facts": {
                    "discovered_interpreter_python": "/usr/bin/python3.12"
                },
                "changed": false,
                "ping": "pong"
            }

         - Cria√ß√£o de playbooks
            A cria√ß√£o dos playbooks eu recomendo criar externamente e depois copiar para dentro do Controller.
            Acesse /ansible/project/playbooks e crie o seu playbook tipo, sudo nano nome-do-seu-playbook

            No projeto tem alguns playbooks essenciais, segue a ordem de execu√ß√£o deles:
            1 - masternode - voc√™ vai preparar o seu node master do k8s
            2 - workers-init - nele voc√™ vai preparar os workers para entrarem no cluster
            3 - joinworker - voc√™ vai adicionar os workers no seu cluster


         - Testes de para verificar sucesso(tudo isso dentro do seu master)

            - kubectl get nodes -o wide
               Retorna a lista de n√≥s do cluster com detalhes adicionais (‚Äúwide‚Äù):
               STATUS: se o n√≥ est√° pronto para rodar pods
               ROLES: fun√ß√£o (control-plane ou worker)
               VERSION: vers√£o do kubelet
               INTERNAL-IP: IP usado para comunica√ß√£o interna
               OS-IMAGE e KERNEL-VERSION
               CONTAINER-RUNTIME (containerd, docker, etc.)

            - kubectl get pods -A
               Lista todos os pods de todos os namespaces (-A = all namespaces).
               √â usado para confirmar:
               Calico (CNI) funcionando
               CoreDNS funcionando
               kube-apiserver, scheduler, controller-manager, etcd no master
               kube-proxy em todos os n√≥s

         - Exemplo de deploy

            - kubectl apply -f https://k8s.io/examples/application/deployment.yaml
               Cria um Deployment de exemplo (nginx) fornecido pela documenta√ß√£o oficial do Kubernetes:
               2 r√©plicas de nginx
               Pod template simples para teste
               Serve para validar o cluster e o CNI.

            - kubectl get pods -n default -w
               Mostra os pods do namespace default e segue assistindo mudan√ßas (watch).
               Permite acompanhar:
               ContainerCreating
               Pull da imagem
               Startup
               Running
            
            - kubectl expose deployment nginx-deployment --type=NodePort --name=nginx
               Cria um Service do tipo NodePort (porta exposta nos nodes) redirecionando para os pods do Deployment.
               NodePort = cria uma porta alta (30000‚Äì32767) acess√≠vel em qualquer n√≥ do cluster.

            - kubectl get svc nginx
               Mostra detalhes do Service nginx:
               CLUSTER-IP: IP interno
               PORT(S): exemplo ‚Üí 80:31500/TCP
               Porta 80 interna ‚Üí Porta 31500 exposta nos nodes

            - curl http://ip-do-worker:31500
               Ele deve retorna a p√°gina do ngnix

   
      


Erros conhecidos:
   - Permission denied (publickey)
      Esse erro prov√©m da falta da chave no Agente que est√° tentando conectar via ssh em outra m√°quina, isso pode acontecer ao tentar usar um playbook em uma m√°quina oua cessar via ssh uma m√°quina de outra m√°quina.

      * Solu√ß√£o:
         -  SSH de uma m√°quina para outra:
            Copie a key.pem gerada pelo projeto nesse caso aqui (ansible_controller_key.pem) para dentro da m√°quina que ira fazer o ssh, nesse caso rode os 
            seguintes comandos para criar no maquina pai:
            
            mkdir -p ~/.ssh
            chmod 700 ~/.ssh
            nano ~/.ssh/id_rsa
         
            e ap√≥s isso cole a chave dentro do arquivo, salve e feche. Como √∫tlimo comando rode?

            chmod 600 ~/.ssh/id_rsa

            depois tente usar o ssh user@ip novamente para conectar

         - Playbook do Ansible para outra m√°quina:
